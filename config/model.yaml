# Model configuration for ScreenSpot TRM
# ========================================

# CLIP backbone configuration
clip_model: "ViT-B-16"
clip_pretrained: "openai"

# TRM controller configuration
# Hidden size for TRM (should be smaller than CLIP dim for efficiency)
trm_hidden_size: 256

# H_cycles: Number of outer deep supervision cycles
# More cycles = more refinement but slower training
# Paper uses 3 for most experiments
H_cycles: 3

# L_cycles: Number of inner latent reasoning cycles per H_cycle
# Controls depth of reasoning within each refinement step
L_cycles: 4

# L_layers: Number of layers in each reasoning module
# Paper uses 2 for efficiency
L_layers: 2

# expansion: SwiGLU expansion factor
# Higher = more capacity but more parameters
expansion: 4.0

# BBox head configuration
bbox_hidden_dim: 128

# bbox_output_format: How to parameterize bbox prediction
# "xyxy" - direct prediction of (x1, y1, x2, y2)
# "cxcywh" - predict (cx, cy, w, h) and convert (can be more stable)
bbox_output_format: "xyxy"

# Fusion configuration
# fusion_type: How to combine image and text embeddings
# "concat_proj" - concatenate and project (default)
# "add_proj" - add and project
fusion_type: "concat_proj"
